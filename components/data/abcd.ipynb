{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa6a1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting healthcare demand prediction model with advanced techniques...\n",
      "Loading dataset...\n",
      "Dataset shape: (200000, 35)\n",
      "\n",
      "Exploring data...\n",
      "\n",
      "Performing advanced feature engineering...\n",
      "Dataset shape after feature engineering: (82611, 107)\n",
      "ER Spikes: 7138 (8.64%)\n",
      "OPD Spikes: 4947 (5.99%)\n",
      "Overall Healthcare Spikes: 11597 (14.04%)\n",
      "\n",
      "Splitting data for training and testing...\n",
      "Training set size: 68408\n",
      "Testing set size: 14203\n",
      "\n",
      "Detecting and removing anomalies...\n",
      "Removed 3421 anomalies (5.00%)\n",
      "Filtered training set size: 64987\n",
      "\n",
      "Training ER visits prediction model with Bayesian Optimization...\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best parameters: OrderedDict({'regressor__colsample_bytree': 0.9349553422213137, 'regressor__learning_rate': 0.1410001814373689, 'regressor__max_depth': 4, 'regressor__n_estimators': 290, 'regressor__subsample': 0.9456511661859803})\n",
      "ER Model - MAE: 0.34, RMSE: 0.48, R²: 0.88\n",
      "\n",
      "Training OPD visits prediction model with Bayesian Optimization...\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best parameters: OrderedDict({'regressor__colsample_bytree': 0.8468317434009265, 'regressor__learning_rate': 0.10204043807287104, 'regressor__max_depth': 4, 'regressor__n_estimators': 275, 'regressor__subsample': 0.8370478701222271})\n",
      "OPD Model - MAE: 0.46, RMSE: 0.58, R²: 0.84\n",
      "\n",
      "Training healthcare demand spike prediction model with Ensemble Learning...\n",
      "Optimal threshold: 0.583\n",
      "Spike Prediction - Accuracy: 0.94, Precision: 0.93, Recall: 0.76, F1: 0.83\n",
      "ROC-AUC: 0.94, PR-AUC: 0.90\n",
      "\n",
      "Training area-specific models...\n",
      "Training model for Western Suburbs...\n",
      "Western Suburbs - Accuracy: 0.95, Precision: 0.92, Recall: 0.76, F1: 0.83, PR-AUC: 0.92\n",
      "Training model for Eastern Suburbs...\n",
      "Eastern Suburbs - Accuracy: 0.94, Precision: 0.95, Recall: 0.75, F1: 0.84, PR-AUC: 0.91\n",
      "Training model for Thane...\n",
      "Thane - Accuracy: 0.95, Precision: 0.95, Recall: 0.79, F1: 0.86, PR-AUC: 0.92\n",
      "Training model for South Mumbai...\n",
      "South Mumbai - Accuracy: 0.95, Precision: 0.86, Recall: 0.83, F1: 0.85, PR-AUC: 0.93\n",
      "Training model for Navi Mumbai...\n",
      "Navi Mumbai - Accuracy: 0.96, Precision: 0.97, Recall: 0.76, F1: 0.85, PR-AUC: 0.93\n",
      "\n",
      "Generating 30-day healthcare demand forecast...\n",
      "\n",
      "No early warnings generated for the forecast period.\n",
      "Forecast saved to 'healthcare_demand_forecast.csv'\n",
      "\n",
      "No high-risk days identified in the forecast period.\n",
      "\n",
      "Healthcare demand prediction model training and forecasting completed!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For preprocessing\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (mean_absolute_error, mean_squared_error, r2_score, \n",
    "                            accuracy_score, precision_score, recall_score, \n",
    "                            f1_score, roc_auc_score, confusion_matrix,\n",
    "                            precision_recall_curve, auc, roc_curve)\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "# For modeling\n",
    "from sklearn.ensemble import (RandomForestRegressor, GradientBoostingRegressor, \n",
    "                             RandomForestClassifier, GradientBoostingClassifier,\n",
    "                             VotingClassifier, StackingClassifier, IsolationForest)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.combine import SMOTETomek\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Starting healthcare demand prediction model with advanced techniques...\")\n",
    "\n",
    "# Load the dataset\n",
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv('mumbai_healthcare_demand_dataset.csv')\n",
    "\n",
    "# Convert date to datetime\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "\n",
    "# Data Exploration\n",
    "print(\"\\nExploring data...\")\n",
    "# Aggregate by date to see daily patterns\n",
    "daily_data = df.groupby('Date')[['ER_Visits', 'OPD_Visits']].sum().reset_index()\n",
    "\n",
    "# Plot daily ER visits\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(daily_data['Date'], daily_data['ER_Visits'])\n",
    "plt.title('Daily ER Visits')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Visits')\n",
    "plt.tight_layout()\n",
    "plt.savefig('daily_er_visits.png')\n",
    "plt.close()\n",
    "\n",
    "# Advanced Feature Engineering\n",
    "print(\"\\nPerforming advanced feature engineering...\")\n",
    "def advanced_feature_engineering(df):\n",
    "    # Create a copy to avoid modifying the original\n",
    "    data = df.copy()\n",
    "    \n",
    "    # Extract date components\n",
    "    data['DayOfWeek'] = data['Date'].dt.dayofweek\n",
    "    data['Month'] = data['Date'].dt.month\n",
    "    data['Year'] = data['Date'].dt.year\n",
    "    data['Day'] = data['Date'].dt.day\n",
    "    data['Quarter'] = data['Date'].dt.quarter\n",
    "    data['IsWeekend'] = (data['DayOfWeek'] >= 5).astype(int)\n",
    "    data['DayOfYear'] = data['Date'].dt.dayofyear\n",
    "    data['WeekOfYear'] = data['Date'].dt.isocalendar().week\n",
    "    \n",
    "    # Create holiday proximity feature (days before/after holiday)\n",
    "    holiday_dates = data[data['IsHoliday'] == 1]['Date'].unique()\n",
    "    data['DaysToHoliday'] = 100  # Default large value\n",
    "    \n",
    "    for date in data['Date'].unique():\n",
    "        days_to_holiday = min([abs((date - pd.Timestamp(hdate)).days) for hdate in holiday_dates], default=100)\n",
    "        data.loc[data['Date'] == date, 'DaysToHoliday'] = days_to_holiday\n",
    "    \n",
    "    # Create seasonal indicators using sine and cosine transforms for cyclical features\n",
    "    data['MonthSin'] = np.sin(2 * np.pi * data['Month']/12)\n",
    "    data['MonthCos'] = np.cos(2 * np.pi * data['Month']/12)\n",
    "    data['DayOfYearSin'] = np.sin(2 * np.pi * data['DayOfYear']/365)\n",
    "    data['DayOfYearCos'] = np.cos(2 * np.pi * data['DayOfYear']/365)\n",
    "    \n",
    "    # Create lag features for time series\n",
    "    for lag in [7, 14, 28]:  # 1 week, 2 weeks, 4 weeks\n",
    "        data[f'ER_Lag_{lag}'] = data.groupby(['Area'])['ER_Visits'].transform(\n",
    "            lambda x: x.shift(lag))\n",
    "        data[f'OPD_Lag_{lag}'] = data.groupby(['Area'])['OPD_Visits'].transform(\n",
    "            lambda x: x.shift(lag))\n",
    "    \n",
    "    # Create rolling averages\n",
    "    for window in [7, 14, 28]:\n",
    "        data[f'ER_RollingMean_{window}'] = data.groupby(['Area'])['ER_Visits'].transform(\n",
    "            lambda x: x.shift(1).rolling(window=window, min_periods=1).mean())\n",
    "        data[f'OPD_RollingMean_{window}'] = data.groupby(['Area'])['OPD_Visits'].transform(\n",
    "            lambda x: x.shift(1).rolling(window=window, min_periods=1).mean())\n",
    "    \n",
    "    # Add moving averages with different windows\n",
    "    for window in [3, 5]:\n",
    "        data[f'ER_MA_{window}'] = data.groupby(['Area'])['ER_Visits'].transform(\n",
    "            lambda x: x.shift(1).rolling(window=window, min_periods=1).mean())\n",
    "        data[f'OPD_MA_{window}'] = data.groupby(['Area'])['OPD_Visits'].transform(\n",
    "            lambda x: x.shift(1).rolling(window=window, min_periods=1).mean())\n",
    "    \n",
    "    # Add exponentially weighted moving averages\n",
    "    data['ER_EWMA'] = data.groupby(['Area'])['ER_Visits'].transform(\n",
    "        lambda x: x.shift(1).ewm(span=7).mean())\n",
    "    data['OPD_EWMA'] = data.groupby(['Area'])['OPD_Visits'].transform(\n",
    "        lambda x: x.shift(1).ewm(span=7).mean())\n",
    "    \n",
    "    # Add lag differences (rate of change)\n",
    "    for lag in [7, 14]:\n",
    "        data[f'ER_Diff_{lag}'] = data['ER_Visits'] - data[f'ER_Lag_{lag}']\n",
    "        data[f'OPD_Diff_{lag}'] = data['OPD_Visits'] - data[f'OPD_Lag_{lag}']\n",
    "    \n",
    "    # Aggregate environmental factors by area and date\n",
    "    env_cols = ['Temperature', 'Humidity', 'AQI', 'Precipitation']\n",
    "    area_date_env = data.groupby(['Area', 'Date'])[env_cols].mean().reset_index()\n",
    "    \n",
    "    # Merge back to get area-level environmental factors\n",
    "    data = data.merge(area_date_env, on=['Area', 'Date'], suffixes=('', '_AreaAvg'))\n",
    "    \n",
    "    # Create interaction features\n",
    "    data['TempHumidityInteraction'] = data['Temperature'] * data['Humidity'] / 100\n",
    "    data['ComorbidityCount'] = (data['HasDiabetes'] + data['HasHypertension'] + \n",
    "                              data['HasAsthma'] + data['HasCOPD'] + data['HasHeartDisease'])\n",
    "    \n",
    "    # Add polynomial features for important numerical variables\n",
    "    for col in ['Age', 'Temperature', 'AQI']:\n",
    "        data[f'{col}_Squared'] = data[col] ** 2\n",
    "    \n",
    "    # Add more interaction terms\n",
    "    data['Age_Temperature'] = data['Age'] * data['Temperature'] / 100\n",
    "    data['AQI_Asthma'] = data['AQI'] * data['HasAsthma']\n",
    "    data['Temp_COPD'] = data['Temperature'] * data['HasCOPD']\n",
    "    data['Humidity_Asthma'] = data['Humidity'] * data['HasAsthma']\n",
    "    \n",
    "    # Create risk score based on multiple factors\n",
    "    data['HealthRiskScore'] = (\n",
    "        data['Age'] / 100 +  # Age factor\n",
    "        data['ComorbidityCount'] * 0.2 +  # Comorbidity factor\n",
    "        (data['Temperature'] > 32).astype(int) * 0.15 +  # High temperature\n",
    "        (data['Humidity'] > 80).astype(int) * 0.1 +  # High humidity\n",
    "        (data['AQI'] > 100).astype(int) * 0.15 +  # Poor air quality\n",
    "        data['IsVectorDiseaseRisk'] * 0.2 +  # Vector disease risk\n",
    "        data['IsFluSeason'] * 0.15 +  # Flu season\n",
    "        data['IsCycloneRisk'] * 0.25  # Cyclone risk\n",
    "    )\n",
    "    \n",
    "    # Create more complex risk scores\n",
    "    data['RespiratoryRiskScore'] = (\n",
    "        data['HasAsthma'] * 2 + \n",
    "        data['HasCOPD'] * 2 + \n",
    "        (data['AQI'] > 100).astype(int) * 1.5 +\n",
    "        (data['PollenCount'] > 100).astype(int) * 1.2\n",
    "    )\n",
    "    \n",
    "    data['CardioRiskScore'] = (\n",
    "        data['HasHeartDisease'] * 2 + \n",
    "        data['HasHypertension'] * 1.5 + \n",
    "        data['HasDiabetes'] * 1.2 +\n",
    "        (data['Age'] > 65).astype(int) * 1.5\n",
    "    )\n",
    "    \n",
    "    # Create area-specific features\n",
    "    data['IsSlum_HighTemp'] = data['IsSlumDwelling'] * (data['Temperature'] > 30).astype(int)\n",
    "    data['Age_Comorbidity'] = data['Age'] * data['ComorbidityCount']\n",
    "    data['SES_Numeric'] = data['SES'].map({\n",
    "        'Low': 0, 'Medium-Low': 1, 'Medium': 2, 'Medium-High': 3, 'High': 4\n",
    "    })\n",
    "    data['SES_Healthcare'] = data['SES_Numeric'] * data['HasPrimaryCare']\n",
    "    \n",
    "    # Create area-specific interaction terms\n",
    "    for area in data['Area'].unique():\n",
    "        # Create area dummy\n",
    "        data[f'Is_{area.replace(\" \", \"_\")}'] = (data['Area'] == area).astype(int)\n",
    "        \n",
    "        # Create area-specific interaction terms\n",
    "        for feature in ['Temperature', 'Humidity', 'AQI']:\n",
    "            data[f'{feature}_{area.replace(\" \", \"_\")}'] = data[feature] * data[f'Is_{area.replace(\" \", \"_\")}']\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Apply advanced feature engineering\n",
    "engineered_df = advanced_feature_engineering(df)\n",
    "\n",
    "# Handle missing values from lag features\n",
    "engineered_df = engineered_df.dropna()\n",
    "print(f\"Dataset shape after feature engineering: {engineered_df.shape}\")\n",
    "\n",
    "# Define healthcare demand spikes\n",
    "def identify_demand_spikes(data, er_threshold_percentile=90, opd_threshold_percentile=90):\n",
    "    # Calculate area-specific thresholds\n",
    "    area_er_thresholds = data.groupby('Area')['ER_Visits'].quantile(er_threshold_percentile/100).to_dict()\n",
    "    area_opd_thresholds = data.groupby('Area')['OPD_Visits'].quantile(opd_threshold_percentile/100).to_dict()\n",
    "    \n",
    "    # Create spike indicators\n",
    "    data['ER_Spike'] = data.apply(lambda x: 1 if x['ER_Visits'] > area_er_thresholds[x['Area']] else 0, axis=1)\n",
    "    data['OPD_Spike'] = data.apply(lambda x: 1 if x['OPD_Visits'] > area_opd_thresholds[x['Area']] else 0, axis=1)\n",
    "    data['Healthcare_Spike'] = ((data['ER_Spike'] + data['OPD_Spike']) > 0).astype(int)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Identify demand spikes\n",
    "spike_df = identify_demand_spikes(engineered_df)\n",
    "\n",
    "# Check distribution of spikes\n",
    "er_spikes = spike_df['ER_Spike'].sum()\n",
    "opd_spikes = spike_df['OPD_Spike'].sum()\n",
    "healthcare_spikes = spike_df['Healthcare_Spike'].sum()\n",
    "\n",
    "print(f\"ER Spikes: {er_spikes} ({spike_df['ER_Spike'].mean()*100:.2f}%)\")\n",
    "print(f\"OPD Spikes: {opd_spikes} ({spike_df['OPD_Spike'].mean()*100:.2f}%)\")\n",
    "print(f\"Overall Healthcare Spikes: {healthcare_spikes} ({spike_df['Healthcare_Spike'].mean()*100:.2f}%)\")\n",
    "\n",
    "# Define features and target variables\n",
    "categorical_features = ['Area', 'Gender', 'SES', 'Insurance', 'Season']\n",
    "numerical_features = [\n",
    "    'Age', 'Temperature', 'Humidity', 'AQI', 'Precipitation', 'PollenCount',\n",
    "    'IsCycloneRisk', 'IsVectorDiseaseRisk', 'IsFluSeason', 'IsHoliday',\n",
    "    'DistanceToHospital', 'HasPrimaryCare', 'HasTransportation',\n",
    "    'IsSlumDwelling', 'HasDiabetes', 'HasHypertension', 'HasAsthma',\n",
    "    'HasCOPD', 'HasHeartDisease', 'DaysToHoliday', 'MonthSin', 'MonthCos',\n",
    "    'DayOfYearSin', 'DayOfYearCos', 'ER_RollingMean_7', 'ER_RollingMean_14', \n",
    "    'ER_RollingMean_28', 'OPD_RollingMean_7', 'OPD_RollingMean_14', 'OPD_RollingMean_28',\n",
    "    'ER_MA_3', 'ER_MA_5', 'OPD_MA_3', 'OPD_MA_5', 'ER_EWMA', 'OPD_EWMA',\n",
    "    'ER_Diff_7', 'ER_Diff_14', 'OPD_Diff_7', 'OPD_Diff_14',\n",
    "    'TempHumidityInteraction', 'ComorbidityCount', 'HealthRiskScore',\n",
    "    'Age_Squared', 'Temperature_Squared', 'AQI_Squared',\n",
    "    'Age_Temperature', 'AQI_Asthma', 'Temp_COPD', 'Humidity_Asthma',\n",
    "    'RespiratoryRiskScore', 'CardioRiskScore',\n",
    "    'IsSlum_HighTemp', 'Age_Comorbidity', 'SES_Numeric', 'SES_Healthcare',\n",
    "    'DayOfWeek', 'Month', 'IsWeekend', 'DayOfYear', 'WeekOfYear'\n",
    "]\n",
    "\n",
    "# Create preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ],\n",
    "    remainder='drop'  # Drop any columns not specified\n",
    ")\n",
    "\n",
    "# Split data for training and testing\n",
    "# Use time-based split since this is time series data\n",
    "print(\"\\nSplitting data for training and testing...\")\n",
    "train_idx = spike_df[spike_df['Date'] < '2024-10-01'].index\n",
    "test_idx = spike_df[spike_df['Date'] >= '2024-10-01'].index\n",
    "\n",
    "X_train = spike_df.loc[train_idx].drop(['ER_Visits', 'OPD_Visits', 'ER_Spike', 'OPD_Spike', \n",
    "                                        'Healthcare_Spike', 'Date', 'PinCode', 'Admission', \n",
    "                                        'LOS_Days', 'IsOverweight', 'IsObese'], axis=1)\n",
    "X_test = spike_df.loc[test_idx].drop(['ER_Visits', 'OPD_Visits', 'ER_Spike', 'OPD_Spike', \n",
    "                                      'Healthcare_Spike', 'Date', 'PinCode', 'Admission', \n",
    "                                      'LOS_Days', 'IsOverweight', 'IsObese'], axis=1)\n",
    "\n",
    "y_er_train = spike_df.loc[train_idx, 'ER_Visits']\n",
    "y_er_test = spike_df.loc[test_idx, 'ER_Visits']\n",
    "y_opd_train = spike_df.loc[train_idx, 'OPD_Visits']\n",
    "y_opd_test = spike_df.loc[test_idx, 'OPD_Visits']\n",
    "y_spike_train = spike_df.loc[train_idx, 'Healthcare_Spike']\n",
    "y_spike_test = spike_df.loc[test_idx, 'Healthcare_Spike']\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Testing set size: {len(X_test)}\")\n",
    "\n",
    "# Detect and remove anomalies\n",
    "print(\"\\nDetecting and removing anomalies...\")\n",
    "isolation_forest = IsolationForest(contamination=0.05, random_state=42)\n",
    "anomalies = isolation_forest.fit_predict(preprocessor.fit_transform(X_train))\n",
    "\n",
    "# Filter out anomalies for training\n",
    "X_train_filtered = X_train[anomalies == 1]\n",
    "y_er_train_filtered = y_er_train[anomalies == 1]\n",
    "y_opd_train_filtered = y_opd_train[anomalies == 1]\n",
    "y_spike_train_filtered = y_spike_train[anomalies == 1]\n",
    "\n",
    "print(f\"Removed {len(X_train) - len(X_train_filtered)} anomalies ({(len(X_train) - len(X_train_filtered))/len(X_train)*100:.2f}%)\")\n",
    "print(f\"Filtered training set size: {len(X_train_filtered)}\")\n",
    "\n",
    "# Model 1: ER Visits Prediction with Bayesian Optimization\n",
    "print(\"\\nTraining ER visits prediction model with Bayesian Optimization...\")\n",
    "\n",
    "# Define search space for ER model\n",
    "er_search_space = {\n",
    "    'regressor__n_estimators': Integer(100, 300),\n",
    "    'regressor__learning_rate': Real(0.01, 0.2, prior='log-uniform'),\n",
    "    'regressor__max_depth': Integer(3, 7),\n",
    "    'regressor__subsample': Real(0.6, 1.0),\n",
    "    'regressor__colsample_bytree': Real(0.6, 1.0)\n",
    "}\n",
    "\n",
    "er_model = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', xgb.XGBRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# Use Bayesian search for hyperparameter optimization\n",
    "er_bayes_search = BayesSearchCV(\n",
    "    er_model,\n",
    "    er_search_space,\n",
    "    n_iter=20,  # Reduced for faster execution\n",
    "    cv=3,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "er_bayes_search.fit(X_train_filtered, y_er_train_filtered)\n",
    "print(f\"Best parameters: {er_bayes_search.best_params_}\")\n",
    "\n",
    "# Use the best model\n",
    "er_model = er_bayes_search.best_estimator_\n",
    "er_preds = er_model.predict(X_test)\n",
    "\n",
    "er_mae = mean_absolute_error(y_er_test, er_preds)\n",
    "er_rmse = np.sqrt(mean_squared_error(y_er_test, er_preds))\n",
    "er_r2 = r2_score(y_er_test, er_preds)\n",
    "\n",
    "print(f\"ER Model - MAE: {er_mae:.2f}, RMSE: {er_rmse:.2f}, R²: {er_r2:.2f}\")\n",
    "\n",
    "# Model 2: OPD Visits Prediction with Bayesian Optimization\n",
    "print(\"\\nTraining OPD visits prediction model with Bayesian Optimization...\")\n",
    "\n",
    "# Define search space for OPD model\n",
    "opd_search_space = {\n",
    "    'regressor__n_estimators': Integer(100, 300),\n",
    "    'regressor__learning_rate': Real(0.01, 0.2, prior='log-uniform'),\n",
    "    'regressor__max_depth': Integer(3, 7),\n",
    "    'regressor__subsample': Real(0.6, 1.0),\n",
    "    'regressor__colsample_bytree': Real(0.6, 1.0)\n",
    "}\n",
    "\n",
    "opd_model = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', xgb.XGBRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# Use Bayesian search for hyperparameter optimization\n",
    "opd_bayes_search = BayesSearchCV(\n",
    "    opd_model,\n",
    "    opd_search_space,\n",
    "    n_iter=20,  # Reduced for faster execution\n",
    "    cv=3,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "opd_bayes_search.fit(X_train_filtered, y_opd_train_filtered)\n",
    "print(f\"Best parameters: {opd_bayes_search.best_params_}\")\n",
    "\n",
    "# Use the best model\n",
    "opd_model = opd_bayes_search.best_estimator_\n",
    "opd_preds = opd_model.predict(X_test)\n",
    "\n",
    "opd_mae = mean_absolute_error(y_opd_test, opd_preds)\n",
    "opd_rmse = np.sqrt(mean_squared_error(y_opd_test, opd_preds))\n",
    "opd_r2 = r2_score(y_opd_test, opd_preds)\n",
    "\n",
    "print(f\"OPD Model - MAE: {opd_mae:.2f}, RMSE: {opd_rmse:.2f}, R²: {opd_r2:.2f}\")\n",
    "\n",
    "# Model 3: Healthcare Demand Spike Prediction with Ensemble Learning\n",
    "print(\"\\nTraining healthcare demand spike prediction model with Ensemble Learning...\")\n",
    "\n",
    "# Calculate class weights based on imbalance\n",
    "class_weight = {0: 1, 1: int(1 / spike_df['Healthcare_Spike'].mean())}\n",
    "\n",
    "# Base models with different strengths\n",
    "base_models = [\n",
    "    ('xgb', xgb.XGBClassifier(\n",
    "        n_estimators=200, \n",
    "        learning_rate=0.05, \n",
    "        max_depth=6, \n",
    "        scale_pos_weight=class_weight[1],\n",
    "        random_state=42\n",
    "    )),\n",
    "    ('rf', RandomForestClassifier(\n",
    "        n_estimators=200, \n",
    "        max_depth=10, \n",
    "        class_weight='balanced',\n",
    "        random_state=42\n",
    "    )),\n",
    "    ('gb', GradientBoostingClassifier(\n",
    "        n_estimators=200, \n",
    "        learning_rate=0.05, \n",
    "        max_depth=4,\n",
    "        random_state=42\n",
    "    ))\n",
    "]\n",
    "\n",
    "# Create a stacking classifier\n",
    "stacking_model = ImbPipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('sampler', SMOTETomek(random_state=42)),\n",
    "    ('classifier', StackingClassifier(\n",
    "        estimators=base_models,\n",
    "        final_estimator=LogisticRegression(class_weight='balanced', max_iter=1000),\n",
    "        cv=5,\n",
    "        stack_method='predict_proba',\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Train the stacking model\n",
    "stacking_model.fit(X_train_filtered, y_spike_train_filtered)\n",
    "spike_probs = stacking_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Find the optimal threshold for classification\n",
    "def find_optimal_threshold(y_true, y_proba):\n",
    "    precisions, recalls, thresholds = precision_recall_curve(y_true, y_proba)\n",
    "    f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-10)\n",
    "    optimal_idx = np.argmax(f1_scores[:-1])  # Exclude the last element which doesn't correspond to a threshold\n",
    "    if len(thresholds) > optimal_idx:\n",
    "        return thresholds[optimal_idx]\n",
    "    else:\n",
    "        return 0.5  # Default threshold\n",
    "\n",
    "# Find optimal threshold\n",
    "optimal_threshold = find_optimal_threshold(y_spike_test, spike_probs)\n",
    "print(f\"Optimal threshold: {optimal_threshold:.3f}\")\n",
    "\n",
    "# Make predictions with optimal threshold\n",
    "spike_preds = (spike_probs >= optimal_threshold).astype(int)\n",
    "\n",
    "# Evaluate spike prediction model\n",
    "accuracy = accuracy_score(y_spike_test, spike_preds)\n",
    "precision = precision_score(y_spike_test, spike_preds)\n",
    "recall = recall_score(y_spike_test, spike_preds)\n",
    "f1 = f1_score(y_spike_test, spike_preds)\n",
    "roc_auc = roc_auc_score(y_spike_test, spike_probs)\n",
    "\n",
    "# Calculate precision-recall AUC (better for imbalanced data)\n",
    "precision_curve, recall_curve, _ = precision_recall_curve(y_spike_test, spike_probs)\n",
    "pr_auc = auc(recall_curve, precision_curve)\n",
    "\n",
    "print(f\"Spike Prediction - Accuracy: {accuracy:.2f}, Precision: {precision:.2f}, Recall: {recall:.2f}, F1: {f1:.2f}\")\n",
    "print(f\"ROC-AUC: {roc_auc:.2f}, PR-AUC: {pr_auc:.2f}\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(y_spike_test, spike_preds)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix for Healthcare Demand Spike Prediction')\n",
    "plt.savefig('confusion_matrix.png')\n",
    "plt.close()\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "fpr, tpr, _ = roc_curve(y_spike_test, spike_probs)\n",
    "plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve for Healthcare Demand Spike Prediction')\n",
    "plt.legend()\n",
    "plt.savefig('roc_curve.png')\n",
    "plt.close()\n",
    "\n",
    "# Plot precision-recall curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall_curve, precision_curve, label=f'PR Curve (AUC = {pr_auc:.2f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve for Healthcare Demand Spike Prediction')\n",
    "plt.legend()\n",
    "plt.savefig('pr_curve.png')\n",
    "plt.close()\n",
    "\n",
    "# Area-specific models with improved approach\n",
    "print(\"\\nTraining area-specific models...\")\n",
    "areas = spike_df['Area'].unique()\n",
    "area_models = {}\n",
    "area_results = {}\n",
    "\n",
    "for area in areas:\n",
    "    print(f\"Training model for {area}...\")\n",
    "    \n",
    "    # Filter data for this area\n",
    "    area_train_idx = spike_df[(spike_df['Area'] == area) & (spike_df['Date'] < '2024-10-01')].index\n",
    "    area_test_idx = spike_df[(spike_df['Area'] == area) & (spike_df['Date'] >= '2024-10-01')].index\n",
    "    \n",
    "    X_area_train = spike_df.loc[area_train_idx].drop(['ER_Visits', 'OPD_Visits', 'ER_Spike', 'OPD_Spike', \n",
    "                                                     'Healthcare_Spike', 'Date', 'PinCode', 'Admission', \n",
    "                                                     'LOS_Days', 'Area', 'IsOverweight', 'IsObese'], axis=1)\n",
    "    X_area_test = spike_df.loc[area_test_idx].drop(['ER_Visits', 'OPD_Visits', 'ER_Spike', 'OPD_Spike', \n",
    "                                                   'Healthcare_Spike', 'Date', 'PinCode', 'Admission', \n",
    "                                                   'LOS_Days', 'Area', 'IsOverweight', 'IsObese'], axis=1)\n",
    "    \n",
    "    y_area_train = spike_df.loc[area_train_idx, 'Healthcare_Spike']\n",
    "    y_area_test = spike_df.loc[area_test_idx, 'Healthcare_Spike']\n",
    "    \n",
    "    # Create area-specific preprocessor (without Area feature)\n",
    "    area_categorical_features = [f for f in categorical_features if f != 'Area']\n",
    "    \n",
    "    area_preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numerical_features),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), area_categorical_features)\n",
    "        ],\n",
    "        remainder='drop'\n",
    "    )\n",
    "    \n",
    "    # Calculate class weights for this area\n",
    "    area_class_weight = {0: 1, 1: int(1 / max(0.01, y_area_train.mean()))}\n",
    "    \n",
    "    # Create and train model with ADASYN for imbalanced classes\n",
    "    area_model = ImbPipeline([\n",
    "        ('preprocessor', area_preprocessor),\n",
    "        ('sampler', ADASYN(random_state=42, sampling_strategy=0.5)),\n",
    "        ('classifier', xgb.XGBClassifier(\n",
    "            n_estimators=200,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=6,\n",
    "            scale_pos_weight=area_class_weight[1],\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=42\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    # Train the model\n",
    "    area_model.fit(X_area_train, y_area_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    area_probs = area_model.predict_proba(X_area_test)[:, 1]\n",
    "    \n",
    "    # Find optimal threshold for this area\n",
    "    if len(np.unique(y_area_test)) > 1 and sum(y_area_test) > 0:\n",
    "        area_threshold = find_optimal_threshold(y_area_test, area_probs)\n",
    "    else:\n",
    "        area_threshold = 0.5\n",
    "    \n",
    "    area_preds = (area_probs >= area_threshold).astype(int)\n",
    "    \n",
    "    area_accuracy = accuracy_score(y_area_test, area_preds)\n",
    "    area_precision = precision_score(y_area_test, area_preds, zero_division=0)\n",
    "    area_recall = recall_score(y_area_test, area_preds, zero_division=0)\n",
    "    area_f1 = f1_score(y_area_test, area_preds, zero_division=0)\n",
    "    \n",
    "    # Calculate precision-recall AUC\n",
    "    if len(np.unique(y_area_test)) > 1 and sum(y_area_test) > 0:  # Only if both classes are present\n",
    "        area_precision_curve, area_recall_curve, _ = precision_recall_curve(y_area_test, area_probs)\n",
    "        area_pr_auc = auc(area_recall_curve, area_precision_curve)\n",
    "    else:\n",
    "        area_pr_auc = 0\n",
    "    \n",
    "    print(f\"{area} - Accuracy: {area_accuracy:.2f}, Precision: {area_precision:.2f}, Recall: {area_recall:.2f}, F1: {area_f1:.2f}, PR-AUC: {area_pr_auc:.2f}\")\n",
    "    \n",
    "    # Store model and results\n",
    "    area_models[area] = area_model\n",
    "    area_results[area] = {\n",
    "        'accuracy': area_accuracy,\n",
    "        'precision': area_precision,\n",
    "        'recall': area_recall,\n",
    "        'f1': area_f1,\n",
    "        'pr_auc': area_pr_auc,\n",
    "        'threshold': area_threshold\n",
    "    }\n",
    "\n",
    "# Function to forecast future healthcare demand\n",
    "def forecast_future_demand(models, last_date, features_df, area_results, days=30):\n",
    "    \"\"\"\n",
    "    Generate healthcare demand forecasts for the next specified number of days\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    models : dict\n",
    "        Dictionary of area-specific models\n",
    "    last_date : datetime\n",
    "        The last date in the dataset\n",
    "    features_df : DataFrame\n",
    "        The dataset used for training\n",
    "    area_results : dict\n",
    "        Dictionary of area-specific results including optimal thresholds\n",
    "    days : int\n",
    "        Number of days to forecast\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame with forecasts for each area\n",
    "    \"\"\"\n",
    "    # Create a date range for the forecast period\n",
    "    future_dates = pd.date_range(start=last_date + timedelta(days=1), periods=days)\n",
    "    \n",
    "    # Create a DataFrame for future dates\n",
    "    future_df = pd.DataFrame({'Date': future_dates})\n",
    "    \n",
    "    # Add date-based features\n",
    "    future_df['DayOfWeek'] = future_df['Date'].dt.dayofweek\n",
    "    future_df['Month'] = future_df['Date'].dt.month\n",
    "    future_df['Year'] = future_df['Date'].dt.year\n",
    "    future_df['Day'] = future_df['Date'].dt.day\n",
    "    future_df['Quarter'] = future_df['Date'].dt.quarter\n",
    "    future_df['IsWeekend'] = (future_df['DayOfWeek'] >= 5).astype(int)\n",
    "    future_df['DayOfYear'] = future_df['Date'].dt.dayofyear\n",
    "    future_df['WeekOfYear'] = future_df['Date'].dt.isocalendar().week\n",
    "    \n",
    "    # Add seasonal indicators\n",
    "    future_df['MonthSin'] = np.sin(2 * np.pi * future_df['Month']/12)\n",
    "    future_df['MonthCos'] = np.cos(2 * np.pi * future_df['Month']/12)\n",
    "    future_df['DayOfYearSin'] = np.sin(2 * np.pi * future_df['DayOfYear']/365)\n",
    "    future_df['DayOfYearCos'] = np.cos(2 * np.pi * future_df['DayOfYear']/365)\n",
    "    future_df['Season'] = future_df['Month'].apply(lambda m: 'Winter' if m in [12, 1, 2] else\n",
    "                                                ('Summer' if m in [3, 4, 5] else\n",
    "                                                 ('Monsoon' if m in [6, 7, 8, 9] else 'Post-Monsoon')))\n",
    "    \n",
    "    # Add holiday indicators\n",
    "    future_df['IsHoliday'] = 0\n",
    "    # Republic Day\n",
    "    future_df.loc[(future_df['Month'] == 1) & (future_df['Day'] == 26), 'IsHoliday'] = 1\n",
    "    # Independence Day\n",
    "    future_df.loc[(future_df['Month'] == 8) & (future_df['Day'] == 15), 'IsHoliday'] = 1\n",
    "    \n",
    "    # Add DaysToHoliday\n",
    "    future_df['DaysToHoliday'] = 100  # Default value\n",
    "    holiday_dates = pd.to_datetime([\n",
    "        f\"{future_df['Year'].iloc[0]}-01-26\",  # Republic Day\n",
    "        f\"{future_df['Year'].iloc[0]}-08-15\",  # Independence Day\n",
    "    ])\n",
    "    \n",
    "    for i, date in enumerate(future_df['Date']):\n",
    "        days_to_holiday = min([abs((date - hdate).days) for hdate in holiday_dates], default=100)\n",
    "        future_df.loc[i, 'DaysToHoliday'] = days_to_holiday\n",
    "    \n",
    "    # Add environmental forecasts based on historical monthly averages\n",
    "    for month in range(1, 13):\n",
    "        month_mask = features_df['Month'] == month\n",
    "        \n",
    "        for col in ['Temperature', 'Humidity', 'AQI', 'Precipitation', 'PollenCount']:\n",
    "            if month_mask.sum() > 0:\n",
    "                future_df.loc[future_df['Month'] == month, col] = features_df.loc[month_mask, col].mean()\n",
    "            else:\n",
    "                # Fallback if no data for this month\n",
    "                future_df.loc[future_df['Month'] == month, col] = features_df[col].mean()\n",
    "    \n",
    "    # Add seasonal disease indicators\n",
    "    future_df['IsFluSeason'] = future_df['Month'].apply(lambda x: 1 if x in [11, 12, 1, 2] else 0)\n",
    "    future_df['IsVectorDiseaseRisk'] = future_df['Month'].apply(lambda x: 1 if x in [6, 7, 8, 9, 10] else 0)\n",
    "    future_df['IsCycloneRisk'] = future_df['Month'].apply(lambda x: 1 if x in [5, 6, 10, 11] else 0)\n",
    "    \n",
    "    # Create predictions for each area\n",
    "    all_predictions = []\n",
    "    \n",
    "    for area in models.keys():\n",
    "        area_df = future_df.copy()\n",
    "        area_df['Area'] = area\n",
    "        \n",
    "        # Add area-specific features (using averages from historical data)\n",
    "        area_mask = features_df['Area'] == area\n",
    "        \n",
    "        # Add demographic and SDOH averages for this area\n",
    "        for col in ['DistanceToHospital', 'HasPrimaryCare', 'HasTransportation', 'IsSlumDwelling',\n",
    "                   'Age', 'HasDiabetes', 'HasHypertension', 'HasAsthma', 'HasCOPD', 'HasHeartDisease']:\n",
    "            if area_mask.sum() > 0:\n",
    "                area_df[col] = features_df.loc[area_mask, col].mean()\n",
    "            else:\n",
    "                area_df[col] = features_df[col].mean()\n",
    "        \n",
    "        # Add categorical distributions\n",
    "        for cat in ['Gender', 'SES', 'Insurance']:\n",
    "            if area_mask.sum() > 0:\n",
    "                cat_dist = features_df.loc[area_mask, cat].value_counts(normalize=True)\n",
    "                area_df[cat] = np.random.choice(cat_dist.index, size=len(area_df), p=cat_dist.values)\n",
    "            else:\n",
    "                # Fallback if no data for this area\n",
    "                cat_dist = features_df[cat].value_counts(normalize=True)\n",
    "                area_df[cat] = np.random.choice(cat_dist.index, size=len(area_df), p=cat_dist.values)\n",
    "        \n",
    "        # Calculate polynomial features\n",
    "        area_df['Age_Squared'] = area_df['Age'] ** 2\n",
    "        area_df['Temperature_Squared'] = area_df['Temperature'] ** 2\n",
    "        area_df['AQI_Squared'] = area_df['AQI'] ** 2\n",
    "        \n",
    "        # Calculate ComorbidityCount\n",
    "        area_df['ComorbidityCount'] = (area_df['HasDiabetes'] + area_df['HasHypertension'] + \n",
    "                                      area_df['HasAsthma'] + area_df['HasCOPD'] + area_df['HasHeartDisease'])\n",
    "        \n",
    "        # Calculate interaction terms\n",
    "        area_df['TempHumidityInteraction'] = area_df['Temperature'] * area_df['Humidity'] / 100\n",
    "        area_df['Age_Temperature'] = area_df['Age'] * area_df['Temperature'] / 100\n",
    "        area_df['AQI_Asthma'] = area_df['AQI'] * area_df['HasAsthma']\n",
    "        area_df['Temp_COPD'] = area_df['Temperature'] * area_df['HasCOPD']\n",
    "        area_df['Humidity_Asthma'] = area_df['Humidity'] * area_df['HasAsthma']\n",
    "        \n",
    "        # Calculate HealthRiskScore\n",
    "        area_df['HealthRiskScore'] = (\n",
    "            area_df['Age'] / 100 +\n",
    "            area_df['ComorbidityCount'] * 0.2 +\n",
    "            (area_df['Temperature'] > 32).astype(int) * 0.15 +\n",
    "            (area_df['Humidity'] > 80).astype(int) * 0.1 +\n",
    "            (area_df['AQI'] > 100).astype(int) * 0.15 +\n",
    "            area_df['IsVectorDiseaseRisk'] * 0.2 +\n",
    "            area_df['IsFluSeason'] * 0.15 +\n",
    "            area_df['IsCycloneRisk'] * 0.25\n",
    "        )\n",
    "        \n",
    "        # Calculate RespiratoryRiskScore\n",
    "        area_df['RespiratoryRiskScore'] = (\n",
    "            area_df['HasAsthma'] * 2 + \n",
    "            area_df['HasCOPD'] * 2 + \n",
    "            (area_df['AQI'] > 100).astype(int) * 1.5 +\n",
    "            (area_df['PollenCount'] > 100).astype(int) * 1.2\n",
    "        )\n",
    "        \n",
    "        # Calculate CardioRiskScore\n",
    "        area_df['CardioRiskScore'] = (\n",
    "            area_df['HasHeartDisease'] * 2 + \n",
    "            area_df['HasHypertension'] * 1.5 + \n",
    "            area_df['HasDiabetes'] * 1.2 +\n",
    "            (area_df['Age'] > 65).astype(int) * 1.5\n",
    "        )\n",
    "        \n",
    "        # Calculate SES_Numeric\n",
    "        area_df['SES_Numeric'] = area_df['SES'].map({\n",
    "            'Low': 0, 'Medium-Low': 1, 'Medium': 2, 'Medium-High': 3, 'High': 4\n",
    "        })\n",
    "        \n",
    "        # Calculate SES_Healthcare\n",
    "        area_df['SES_Healthcare'] = area_df['SES_Numeric'] * area_df['HasPrimaryCare']\n",
    "        \n",
    "        # Calculate IsSlum_HighTemp\n",
    "        area_df['IsSlum_HighTemp'] = area_df['IsSlumDwelling'] * (area_df['Temperature'] > 30).astype(int)\n",
    "        \n",
    "        # Calculate Age_Comorbidity\n",
    "        area_df['Age_Comorbidity'] = area_df['Age'] * area_df['ComorbidityCount']\n",
    "        \n",
    "        # Create area-specific interaction terms\n",
    "        for a in features_df['Area'].unique():\n",
    "            area_df[f'Is_{a.replace(\" \", \"_\")}'] = (area_df['Area'] == a).astype(int)\n",
    "            \n",
    "            for feature in ['Temperature', 'Humidity', 'AQI']:\n",
    "                area_df[f'{feature}_{a.replace(\" \", \"_\")}'] = area_df[feature] * area_df[f'Is_{a.replace(\" \", \"_\")}']\n",
    "        \n",
    "        # Add rolling means from the last available data\n",
    "        last_month_data = features_df[(features_df['Area'] == area)].sort_values('Date').tail(30)\n",
    "        \n",
    "        if len(last_month_data) > 0:\n",
    "            for window in [7, 14, 28]:\n",
    "                area_df[f'ER_RollingMean_{window}'] = last_month_data['ER_Visits'].tail(window).mean()\n",
    "                area_df[f'OPD_RollingMean_{window}'] = last_month_data['OPD_Visits'].tail(window).mean()\n",
    "            \n",
    "            for window in [3, 5]:\n",
    "                area_df[f'ER_MA_{window}'] = last_month_data['ER_Visits'].tail(window).mean()\n",
    "                area_df[f'OPD_MA_{window}'] = last_month_data['OPD_Visits'].tail(window).mean()\n",
    "            \n",
    "            area_df['ER_EWMA'] = last_month_data['ER_Visits'].tail(7).mean()  # Simplified EWMA\n",
    "            area_df['OPD_EWMA'] = last_month_data['OPD_Visits'].tail(7).mean()  # Simplified EWMA\n",
    "            \n",
    "            for lag in [7, 14]:\n",
    "                if len(last_month_data) > lag:\n",
    "                    area_df[f'ER_Diff_{lag}'] = last_month_data['ER_Visits'].iloc[-1] - last_month_data['ER_Visits'].iloc[-lag-1]\n",
    "                    area_df[f'OPD_Diff_{lag}'] = last_month_data['OPD_Visits'].iloc[-1] - last_month_data['OPD_Visits'].iloc[-lag-1]\n",
    "                else:\n",
    "                    area_df[f'ER_Diff_{lag}'] = 0\n",
    "                    area_df[f'OPD_Diff_{lag}'] = 0\n",
    "        else:\n",
    "            # Fallback if no data for this area\n",
    "            for window in [7, 14, 28]:\n",
    "                area_df[f'ER_RollingMean_{window}'] = features_df['ER_Visits'].mean()\n",
    "                area_df[f'OPD_RollingMean_{window}'] = features_df['OPD_Visits'].mean()\n",
    "            \n",
    "            for window in [3, 5]:\n",
    "                area_df[f'ER_MA_{window}'] = features_df['ER_Visits'].mean()\n",
    "                area_df[f'OPD_MA_{window}'] = features_df['OPD_Visits'].mean()\n",
    "            \n",
    "            area_df['ER_EWMA'] = features_df['ER_Visits'].mean()\n",
    "            area_df['OPD_EWMA'] = features_df['OPD_Visits'].mean()\n",
    "            \n",
    "            for lag in [7, 14]:\n",
    "                area_df[f'ER_Diff_{lag}'] = 0\n",
    "                area_df[f'OPD_Diff_{lag}'] = 0\n",
    "        \n",
    "        # Use the area-specific model to predict\n",
    "        X_area = area_df.drop(['Date'], axis=1)\n",
    "        \n",
    "        try:\n",
    "            # Make predictions\n",
    "            area_df['Spike_Probability'] = models[area].predict_proba(X_area)[:, 1]\n",
    "            # Use area-specific optimal threshold\n",
    "            area_threshold = area_results[area]['threshold']\n",
    "            area_df['Predicted_Spike'] = (area_df['Spike_Probability'] >= area_threshold).astype(int)\n",
    "        except Exception as e:\n",
    "            print(f\"Error predicting for {area}: {e}\")\n",
    "            # Fallback\n",
    "            area_df['Spike_Probability'] = 0.5\n",
    "            area_df['Predicted_Spike'] = 0\n",
    "        \n",
    "        all_predictions.append(area_df)\n",
    "    \n",
    "    # Combine all area predictions\n",
    "    forecast_df = pd.concat(all_predictions, ignore_index=True)\n",
    "    \n",
    "    return forecast_df\n",
    "\n",
    "# Function to generate early warnings\n",
    "def generate_early_warnings(forecast_df, threshold=0.7, consecutive_days=2):\n",
    "    \"\"\"Generate early warnings for healthcare demand spikes\"\"\"\n",
    "    warnings = []\n",
    "    \n",
    "    for area in forecast_df['Area'].unique():\n",
    "        area_forecast = forecast_df[forecast_df['Area'] == area]\n",
    "        \n",
    "        # Group by date and check for consecutive days above threshold\n",
    "        high_risk_dates = area_forecast[area_forecast['Spike_Probability'] > threshold]['Date'].dt.date.unique()\n",
    "        \n",
    "        # Check for consecutive days\n",
    "        for i in range(len(high_risk_dates) - consecutive_days + 1):\n",
    "            date_sequence = [high_risk_dates[i] + timedelta(days=j) for j in range(consecutive_days)]\n",
    "            if all(d in high_risk_dates for d in date_sequence):\n",
    "                warnings.append({\n",
    "                    'Area': area,\n",
    "                    'Start_Date': high_risk_dates[i],\n",
    "                    'End_Date': high_risk_dates[i + consecutive_days - 1],\n",
    "                    'Duration': consecutive_days,\n",
    "                    'Average_Probability': area_forecast[\n",
    "                        (area_forecast['Date'].dt.date >= high_risk_dates[i]) & \n",
    "                        (area_forecast['Date'].dt.date <= high_risk_dates[i + consecutive_days - 1])\n",
    "                    ]['Spike_Probability'].mean()\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(warnings)\n",
    "\n",
    "# Get the last date in the dataset\n",
    "last_date = spike_df['Date'].max()\n",
    "\n",
    "# Generate forecast for the next 30 days\n",
    "try:\n",
    "    print(\"\\nGenerating 30-day healthcare demand forecast...\")\n",
    "    forecast = forecast_future_demand(area_models, last_date, spike_df, area_results, days=30)\n",
    "    \n",
    "    # Visualize the forecast\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    for area in areas:\n",
    "        area_forecast = forecast[forecast['Area'] == area]\n",
    "        plt.plot(area_forecast['Date'], area_forecast['Spike_Probability'], label=area)\n",
    "    \n",
    "    plt.axhline(y=0.5, color='r', linestyle='--', label='Spike Threshold')\n",
    "    plt.title('30-Day Healthcare Demand Spike Forecast by Area')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Spike Probability')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('forecast_by_area.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Generate early warnings\n",
    "    warnings = generate_early_warnings(forecast, threshold=0.7, consecutive_days=2)\n",
    "    \n",
    "    if len(warnings) > 0:\n",
    "        print(\"\\nEarly Warning System - Healthcare Demand Spike Alerts:\")\n",
    "        for _, row in warnings.iterrows():\n",
    "            print(f\"{row['Area']}: {row['Start_Date']} to {row['End_Date']} ({row['Duration']} days) - Avg. Probability: {row['Average_Probability']:.2f}\")\n",
    "    else:\n",
    "        print(\"\\nNo early warnings generated for the forecast period.\")\n",
    "    \n",
    "    # Save forecast to CSV\n",
    "    forecast.to_csv('healthcare_demand_forecast.csv', index=False)\n",
    "    print(\"Forecast saved to 'healthcare_demand_forecast.csv'\")\n",
    "    \n",
    "    # Identify high-risk days\n",
    "    high_risk_days = forecast[forecast['Spike_Probability'] > 0.7].groupby('Date')['Area'].apply(list).reset_index()\n",
    "    if len(high_risk_days) > 0:\n",
    "        print(\"\\nHigh-risk days for healthcare demand spikes:\")\n",
    "        for _, row in high_risk_days.iterrows():\n",
    "            print(f\"{row['Date'].strftime('%Y-%m-%d')}: {', '.join(row['Area'])}\")\n",
    "    else:\n",
    "        print(\"\\nNo high-risk days identified in the forecast period.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error generating forecast: {e}\")\n",
    "    print(\"Please check that all required features are available in the dataset.\")\n",
    "\n",
    "print(\"\\nHealthcare demand prediction model training and forecasting completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1ef45f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting threadpoolctl==3.1.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl.metadata (9.2 kB)\n",
      "Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: threadpoolctl\n",
      "  Attempting uninstall: threadpoolctl\n",
      "    Found existing installation: threadpoolctl 2.2.0\n",
      "    Uninstalling threadpoolctl-2.2.0:\n",
      "      Successfully uninstalled threadpoolctl-2.2.0\n",
      "Successfully installed threadpoolctl-3.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install threadpoolctl==3.1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90b3945d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-optimize\n",
      "  Downloading scikit_optimize-0.10.2-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-optimize) (1.4.2)\n",
      "Collecting pyaml>=16.9 (from scikit-optimize)\n",
      "  Downloading pyaml-25.1.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy>=1.20.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-optimize) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-optimize) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-optimize) (1.4.2)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-optimize) (23.2)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pyaml>=16.9->scikit-optimize) (6.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0.0->scikit-optimize) (3.1.0)\n",
      "Downloading scikit_optimize-0.10.2-py2.py3-none-any.whl (107 kB)\n",
      "   ---------------------------------------- 0.0/107.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/107.8 kB ? eta -:--:--\n",
      "   ------- -------------------------------- 20.5/107.8 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 102.4/107.8 kB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 107.8/107.8 kB 1.0 MB/s eta 0:00:00\n",
      "Downloading pyaml-25.1.0-py3-none-any.whl (26 kB)\n",
      "Installing collected packages: pyaml, scikit-optimize\n",
      "Successfully installed pyaml-25.1.0 scikit-optimize-0.10.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52953c5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
